"""
Motivation 2 Experiment – Synthetic FD Dataset Generator
=========================================================
Generates a 10,000-row synthetic dataset with 9 features + 1 binary label,
designed with a precisely controlled Functional Dependency (FD) structure for
benchmarking FD-discovery algorithms such as HyFD and TANE.

Ground-truth FD structure
--------------------------
  Cluster A  –  Arithmetic mappings:
    F1 → F2     F2 = 3·F1 + 7            (linear, injective)
    F1 → F3     F3 = F1² mod 100         (non-linear, many-to-one)

  Cluster B  –  Composite key:
    {F4, F5} → F6   F6 = F4·51 + F5      (bijective over [0,50]²)
    NOTE: F4 not-> F6  and  F5 not-> F6  individually.

  Cluster C  –  Transitive dependencies:
    F7 → F8     F8 = tier(F7) ∈ {Low, Medium, High}   (categorical)
    F8 → F9     F9 = ordinal(F8) ∈ {0, 1, 2}
    => F7 → F9  by transitivity

  Cluster D  –  Independent noise (no FDs, no accidental superkeys):
    F10  Bernoulli(p=0.5)        →  2 unique values  {0, 1}
    F11  uniform categorical     →  3 unique values  {0, 1, 2}
    F12  skewed categorical      →  3 unique values  {0, 1, 2}, p=[.5,.3,.2]

    Design rationale:
      The highest-cardinality independent column in the dataset is F6 with
      ≈ 2,548 distinct values.  For a (noise, core) pair to be an accidental
      superkey it must produce ≥ 10,000 distinct joint values.  With each
      noise column capped at cardinality c ≤ 3:
        c × 2,548 ≤ 3 × 2,548 = 7,644 < 10,000
      → no (noise, core) pair can ever be a superkey.
      The triple (F10, F11, F12) has ≤ 2×3×3 = 18 combinations → also safe.
      Result: no noise combination can accidentally determine another column.

  Label:
    {F2, F6} → Y    Y = (F2 + F6) % 2    (binary classification target)
    NOTE: F2 not-> Y  and  F6 not-> Y  individually.
"""

from __future__ import annotations

import os

import numpy as np
import pandas as pd

# ── Configuration ─────────────────────────────────────────────────────────────
SEED = 42
N_ROWS = 10_000

_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_PATH = os.path.join(_DIR, "synthetic_fd_dataset.csv")

# ── Helpers ───────────────────────────────────────────────────────────────────
_TIER_THRESHOLDS = (334, 667)           # partition [0, 1000] into Low/Medium/High
_TIER_MAP: dict[str, int] = {"Low": 0, "Medium": 1, "High": 2}


def _tier(value: int) -> str:
    """Map an integer in [0, 1000] to a categorical tier string."""
    if value < _TIER_THRESHOLDS[0]:
        return "Low"
    elif value < _TIER_THRESHOLDS[1]:
        return "Medium"
    return "High"


def _tier_encode(tier: str) -> int:
    """Ordinal encode a tier string to an integer."""
    return _TIER_MAP[tier]


# ── Cluster generators ────────────────────────────────────────────────────────
def _cluster_a(rng: np.random.Generator, n: int) -> pd.DataFrame:
    """
    F1 seed column: ~500 unique integers sampled with replacement.
    F2 = 3·F1 + 7     → F1 → F2
    F3 = F1² mod 100  → F1 → F3  (many-to-one: multiple F1 values share an F3)
    """
    f1 = rng.integers(0, 500, size=n)
    f2 = f1 * 3 + 7
    f3 = (f1.astype(np.int64) ** 2) % 100
    return pd.DataFrame({"F1": f1, "F2": f2, "F3": f3})


def _cluster_b(rng: np.random.Generator, n: int) -> pd.DataFrame:
    """
    F4 ∈ [0, 50], F5 ∈ [0, 50]: independent categoricals.
    F6 = F4·51 + F5   → {F4, F5} → F6  (bijective for the given domain)
    Single-column FDs F4→F6 and F5→F6 do NOT hold.
    """
    f4 = rng.integers(0, 51, size=n)
    f5 = rng.integers(0, 51, size=n)
    f6 = f4 * 51 + f5
    return pd.DataFrame({"F4": f4, "F5": f5, "F6": f6})


def _cluster_c(rng: np.random.Generator, n: int) -> pd.DataFrame:
    """
    F7 ∈ [0, 1000]: integer base attribute.
    F8 = tier(F7) ∈ {Low, Medium, High}  → F7 → F8
    F9 = ordinal(F8) ∈ {0, 1, 2}         → F8 → F9  ⇒  F7 → F9
    """
    f7 = rng.integers(0, 1001, size=n)
    f8 = np.array([_tier(v) for v in f7])
    f9 = np.array([_tier_encode(t) for t in f8], dtype=np.int64)
    return pd.DataFrame({"F7": f7, "F8": f8, "F9": f9})


def _cluster_d(rng: np.random.Generator, n: int) -> pd.DataFrame:
    """
    F10, F11, F12: three independent low-cardinality noise columns, each
    generated by a distinct distributional rule.

      F10  Bernoulli(p=0.5)           ->  binary   {0, 1}
      F11  Uniform categorical        ->  ternary  {0, 1, 2}
      F12  Skewed categorical         ->  ternary  {0, 1, 2}, p=[0.5, 0.3, 0.2]

    With max column cardinality = 3 and the highest independent core-column
    cardinality being ≈ 2,548 (F6), the product 3 × 2,548 = 7,644 < 10,000,
    so no (noise, core) pair can be an accidental superkey.
    """
    f10 = rng.integers(0, 2, size=n)                             # Bernoulli {0,1}
    f11 = rng.integers(0, 3, size=n)                             # uniform ternary
    f12 = rng.choice([0, 1, 2], p=[0.5, 0.3, 0.2], size=n)     # skewed ternary
    return pd.DataFrame({"F10": f10, "F11": f11, "F12": f12})


def _label(f2: pd.Series, f6: pd.Series) -> pd.Series:
    """
    Y = (F2 + F6) % 2  →  {F2, F6} → Y  (binary target).
    Neither F2 alone nor F6 alone determines Y.
    """
    return ((f2 + f6) % 2).astype(np.int8).rename("Y")


# ── Dataset assembly ──────────────────────────────────────────────────────────
def build_dataset(n: int = N_ROWS, seed: int = SEED) -> pd.DataFrame:
    """
    Build and return the clean synthetic FD dataset as a DataFrame.

    Parameters
    ----------
    n : int
        Number of rows (default 10,000).
    seed : int
        Random seed for reproducibility (default 42).

    Returns
    -------
    pd.DataFrame
        DataFrame with columns F1–F12 and Y (13 columns total).
    """
    rng = np.random.default_rng(seed)
    a = _cluster_a(rng, n)
    b = _cluster_b(rng, n)
    c = _cluster_c(rng, n)
    d = _cluster_d(rng, n)
    y = _label(a["F2"], b["F6"])
    return pd.concat([a, b, c, d, y], axis=1)


# ── Verification ──────────────────────────────────────────────────────────────
def verify_fds(df: pd.DataFrame) -> None:
    """
    Assert that every declared FD holds perfectly in *df*, and that the noise
    columns carry no spurious single-column FD from F1.

    Raises
    ------
    AssertionError
        If any declared FD is violated or a noise column is unexpectedly
        determined by a single column.
    """
    def _check(lhs: list[str], rhs: str) -> None:
        max_distinct = df.groupby(lhs)[rhs].nunique().max()
        assert max_distinct == 1, (
            f"FD VIOLATED  {lhs} -> {rhs}  "
            f"(max distinct RHS values per LHS group = {max_distinct})"
        )
        print(f"  [OK]  {lhs} -> {rhs!r}")

    declared_fds: list[tuple[list[str], str]] = [
        (["F1"],       "F2"),
        (["F1"],       "F3"),
        (["F4", "F5"], "F6"),
        (["F7"],       "F8"),
        (["F8"],       "F9"),
        (["F7"],       "F9"),   # transitive
        (["F2", "F6"], "Y"),
    ]

    print("\n  Declared FD checks:")
    for lhs, rhs in declared_fds:
        _check(lhs, rhs)

    # Verify noise columns have no spurious FDs from F1
    print("\n  Noise column checks (bounded cardinality, no accidental superkeys):")
    for col in ("F10", "F11", "F12"):
        n_unique = df[col].nunique()
        max_distinct = df.groupby("F1")[col].nunique().max()
        assert max_distinct > 1, (
            f"Unexpected FD: F1 -> {col}  (noise column appears deterministic)"
        )
        print(f"  [OK]  {col}: {n_unique} unique values, "
              f"F1 -/-> {col} (max distinct per group = {max_distinct})")


# ── Summary ───────────────────────────────────────────────────────────────────
def print_summary(df: pd.DataFrame) -> None:
    """Print a concise overview: shape, cardinality, label split, FD checks."""
    sep = "=" * 60
    print(f"\n{sep}")
    print("Dataset Summary")
    print(sep)

    print(f"  Shape : {df.shape[0]:,} rows x {df.shape[1]} columns")

    print("\n  Column cardinality:")
    for col in df.columns:
        n_unique = df[col].nunique()
        dtype = df[col].dtype
        print(f"    {col:<5}  {n_unique:>6,} unique   dtype={dtype}")

    print("\n  Label (Y) distribution:")
    for label, count in df["Y"].value_counts().sort_index().items():
        pct = 100 * count / len(df)
        print(f"    Y={label}  ->  {count:,} rows ({pct:.1f} %)")

    verify_fds(df)
    print(sep + "\n")


# ── Entry point ───────────────────────────────────────────────────────────────
def main() -> None:
    print(f"Generating synthetic FD dataset  (n={N_ROWS:,}, seed={SEED}) ...")
    df = build_dataset()
    os.makedirs(_DIR, exist_ok=True)
    df.to_csv(OUTPUT_PATH, index=False)
    print(f"Saved  ->  {OUTPUT_PATH}")
    print_summary(df)


if __name__ == "__main__":
    main()

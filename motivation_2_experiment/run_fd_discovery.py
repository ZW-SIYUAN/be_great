"""
Motivation 2 Experiment – FD Discovery Benchmark
=================================================
Runs HyFD and TANE (local pure-Python implementations in fd_algorithms/)
on the synthetic FD dataset generated by generate_dataset.py, and evaluates
their output against the designed ground-truth FD structure.

Evaluation taxonomy
-------------------
Every discovered FD is classified into exactly one of three categories:

  designed_hit    – Present in the designed ground truth (what we intended the
                    algorithm to find).
  implicit_valid  – Not in the designed ground truth, but verifiably holds in
                    the data (e.g. reverse bijective FDs like F2→F1, F9→F8).
                    These are mathematically correct and arise naturally from
                    the dataset construction; a perfect algorithm SHOULD find
                    them.
  false_positive  – The FD is reported but does NOT hold in the data.
                    This should never occur with exact, sound algorithms.

Metrics are computed with respect to the designed ground truth only:

  Precision = designed_hits / (designed_hits + false_positives)
  Recall    = designed_hits / |ground_truth|
  F1        = harmonic mean of Precision and Recall

Usage
-----
  python run_fd_discovery.py                   # default: runs both algorithms
  python run_fd_discovery.py --algorithms HyFD # run HyFD only
  python run_fd_discovery.py --help
"""

from __future__ import annotations

import argparse
import csv
import sys
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Callable, Sequence

import pandas as pd

# ── Local implementations ─────────────────────────────────────────────────────
_HERE = Path(__file__).parent
sys.path.insert(0, str(_HERE))

from fd_algorithms import hyfd, tane

# ── Paths ─────────────────────────────────────────────────────────────────────
DATA_PATH    = _HERE / "synthetic_fd_dataset.csv"
RESULTS_PATH = _HERE / "fd_benchmark_results.csv"

# ── Ground-truth FDs (designed) ───────────────────────────────────────────────
# Each entry is (frozenset_of_lhs_column_names, rhs_column_name).
# These are the FDs explicitly encoded in generate_dataset.py.
GROUND_TRUTH: list[tuple[frozenset[str], str]] = [
    # Cluster A – arithmetic
    (frozenset({"F1"}),       "F2"),   # F2 = 3·F1 + 7  (injective)
    (frozenset({"F1"}),       "F3"),   # F3 = F1² mod 100  (many-to-one)
    # Cluster B – composite key
    (frozenset({"F4", "F5"}), "F6"),   # F6 = F4·51 + F5
    # Cluster C – transitive chain
    (frozenset({"F7"}),       "F8"),   # tier(F7) ∈ {Low, Medium, High}
    (frozenset({"F8"}),       "F9"),   # ordinal(F8) ∈ {0, 1, 2}
    (frozenset({"F7"}),       "F9"),   # by transitivity
    # Label
    (frozenset({"F2", "F6"}), "Y"),    # Y = (F2 + F6) % 2
]

# Pre-built set for O(1) membership tests
_GT_SET: frozenset[tuple[frozenset[str], str]] = frozenset(
    (lhs, rhs) for lhs, rhs in GROUND_TRUTH
)


# ── Data structures ───────────────────────────────────────────────────────────

@dataclass(frozen=True, order=False)
class FD:
    """Immutable, hashable representation of a Functional Dependency X → A."""

    lhs: frozenset[str]
    rhs: str

    def __str__(self) -> str:
        lhs_str = ", ".join(sorted(self.lhs))
        return f"{{{lhs_str}}} -> {self.rhs}"

    def sort_key(self) -> tuple:
        return (len(self.lhs), tuple(sorted(self.lhs)), self.rhs)

    @classmethod
    def from_raw(cls, lhs: frozenset[str], rhs: str) -> "FD":
        return cls(lhs=lhs, rhs=rhs)


@dataclass
class AlgoResult:
    """Complete output of running one FD-discovery algorithm."""

    name: str
    discovered:      list[FD] = field(default_factory=list)
    designed_hits:   list[FD] = field(default_factory=list)
    implicit_valid:  list[FD] = field(default_factory=list)
    false_positives: list[FD] = field(default_factory=list)
    missed_gt:       list[tuple[frozenset[str], str]] = field(default_factory=list)
    algo_stats:      dict = field(default_factory=dict)
    total_s:         float = 0.0

    # ── Derived metrics ───────────────────────────────────────────────────────
    @property
    def n_designed_hits(self) -> int:  return len(self.designed_hits)
    @property
    def n_implicit(self) -> int:       return len(self.implicit_valid)
    @property
    def n_fp(self) -> int:             return len(self.false_positives)
    @property
    def n_missed(self) -> int:         return len(self.missed_gt)

    @property
    def precision(self) -> float:
        denom = self.n_designed_hits + self.n_fp
        return self.n_designed_hits / denom if denom > 0 else 0.0

    @property
    def recall(self) -> float:
        n_gt = len(GROUND_TRUTH)
        return self.n_designed_hits / n_gt if n_gt > 0 else 0.0

    @property
    def f1(self) -> float:
        p, r = self.precision, self.recall
        return 2 * p * r / (p + r) if (p + r) > 0 else 0.0


# ── Core helpers ──────────────────────────────────────────────────────────────

def verify_fd(df: pd.DataFrame, lhs: frozenset[str], rhs: str) -> bool:
    """
    Return True iff lhs → rhs holds in df (partition-based verification).

    Uses pandas groupby to check that every group sharing the same LHS values
    has exactly one distinct RHS value.  This is the ground-truth oracle used
    to classify 'implicit_valid' FDs discovered by the algorithms.
    """
    return int(df.groupby(list(lhs))[rhs].nunique().max()) == 1


def run_algorithm(
    discover_fn: Callable,
    df: pd.DataFrame,
    kwargs: dict,
) -> tuple[list[FD], float, dict]:
    """
    Call a discovery function and return (FD list, wall-clock seconds, stats).

    Parameters
    ----------
    discover_fn : callable
        tane.discover(df)  or  hyfd.discover(df, n_sample_pairs=N, seed=S)
    df : pd.DataFrame
        The dataset.
    kwargs : dict
        Extra keyword arguments forwarded to discover_fn.

    Returns
    -------
    fds     : list[FD]
    elapsed : float     wall-clock seconds (discovery only)
    stats   : dict      algorithm-internal counters
    """
    t0 = time.perf_counter()
    raw_fds, stats = discover_fn(df, **kwargs)
    elapsed = time.perf_counter() - t0

    fds = [FD.from_raw(lhs, rhs) for lhs, rhs in raw_fds]
    return fds, elapsed, stats


def classify(
    discovered: list[FD],
    df: pd.DataFrame,
) -> tuple[list[FD], list[FD], list[FD], list[tuple[frozenset[str], str]]]:
    """
    Partition discovered FDs into the three evaluation categories.

    Returns
    -------
    designed_hits   : FDs matching the designed ground truth
    implicit_valid  : Additional FDs that hold in the data (not in GT)
    false_positives : FDs that do NOT hold in the data (algorithm error)
    missed_gt       : Designed-GT FDs absent from discovered
    """
    discovered_keys = {(fd.lhs, fd.rhs) for fd in discovered}

    designed_hits:   list[FD] = []
    implicit_valid:  list[FD] = []
    false_positives: list[FD] = []

    for fd in discovered:
        key = (fd.lhs, fd.rhs)
        if key in _GT_SET:
            designed_hits.append(fd)
        elif verify_fd(df, fd.lhs, fd.rhs):
            implicit_valid.append(fd)
        else:
            false_positives.append(fd)

    missed_gt = [
        (lhs, rhs)
        for lhs, rhs in GROUND_TRUTH
        if (lhs, rhs) not in discovered_keys
    ]

    return designed_hits, implicit_valid, false_positives, missed_gt


# ── Reporting ─────────────────────────────────────────────────────────────────
_SEP = "=" * 70


def _fmt_fd(lhs: frozenset[str], rhs: str) -> str:
    return "{" + ", ".join(sorted(lhs)) + "} -> " + rhs


def print_result(r: AlgoResult) -> None:
    """Print a detailed per-algorithm report."""
    print(f"\n{_SEP}")
    print(f"  Algorithm  : {r.name}")
    print(_SEP)
    print(f"  Total time : {r.total_s:.3f} s")
    print(f"  Discovered : {len(r.discovered)} FDs total")
    print()

    # ── Summary metrics ───────────────────────────────────────────────────────
    rows = [
        ("Designed GT hits",            f"{r.n_designed_hits} / {len(GROUND_TRUTH)}"),
        ("Implicit valid FDs",           str(r.n_implicit)),
        ("False positives",              str(r.n_fp)),
        ("Missed (false negatives)",     str(r.n_missed)),
        ("Precision  (wrt designed GT)", f"{r.precision:.4f}"),
        ("Recall     (wrt designed GT)", f"{r.recall:.4f}"),
        ("F1-score   (wrt designed GT)", f"{r.f1:.4f}"),
    ]
    for label, val in rows:
        print(f"  {label:<36}  {val:>10}")

    # ── Algorithm-internal statistics ─────────────────────────────────────────
    if r.algo_stats:
        print()
        print("  Internal algorithm counters:")
        for key, val in r.algo_stats.items():
            label = key.replace("_", " ").capitalize()
            print(f"    {label:<34}  {val:>10}")

    # ── Per-FD breakdown ──────────────────────────────────────────────────────
    print()
    if r.designed_hits:
        print("  [OK] Designed ground-truth FDs discovered:")
        for fd in sorted(r.designed_hits, key=lambda x: x.sort_key()):
            print(f"         {fd}")

    if r.implicit_valid:
        print()
        print("  [~~] Implicit valid FDs (hold in data; not in designed GT):")
        for fd in sorted(r.implicit_valid, key=lambda x: x.sort_key()):
            print(f"         {fd}")

    if r.false_positives:
        print()
        print("  [!!] FALSE POSITIVES - algorithm error:")
        for fd in sorted(r.false_positives, key=lambda x: x.sort_key()):
            print(f"         {fd}  *** VIOLATION ***")

    if r.missed_gt:
        print()
        print("  [--] Missed ground-truth FDs:")
        for lhs, rhs in sorted(r.missed_gt, key=lambda x: (sorted(x[0]), x[1])):
            print(f"         {_fmt_fd(lhs, rhs)}")

    print(_SEP)


def print_comparison(results: list[AlgoResult]) -> None:
    """Print a side-by-side head-to-head comparison table."""
    names = [r.name for r in results]
    col_w = 14

    print(f"\n{'=' * 70}")
    print("  Head-to-Head Comparison")
    print(f"{'=' * 70}")

    header = f"  {'Metric':<38}" + "".join(f"  {n:>{col_w}}" for n in names)
    print(header)
    print(f"  {'-' * (38 + (col_w + 2) * len(results))}")

    def row(label: str, vals: Sequence) -> str:
        return f"  {label:<38}" + "".join(f"  {str(v):>{col_w}}" for v in vals)

    metrics: list[tuple[str, list]] = [
        ("Total time (s)",               [f"{r.total_s:.3f}"   for r in results]),
        ("Total FDs discovered",         [len(r.discovered)    for r in results]),
        ("Designed GT hits",             [f"{r.n_designed_hits}/{len(GROUND_TRUTH)}" for r in results]),
        ("Implicit valid FDs",           [r.n_implicit         for r in results]),
        ("False positives",              [r.n_fp               for r in results]),
        ("Missed (false negatives)",     [r.n_missed           for r in results]),
        ("Precision",                    [f"{r.precision:.4f}" for r in results]),
        ("Recall",                       [f"{r.recall:.4f}"    for r in results]),
        ("F1-score",                     [f"{r.f1:.4f}"        for r in results]),
        ("-- Internal counters --",      [""] * len(results)),
        ("FD checks (PLI calls)",        [r.algo_stats.get("n_fd_checks", "-")       for r in results]),
        ("SP products computed",         [r.algo_stats.get("n_sp_products", "-")     for r in results]),
        ("Candidates pruned (keys)",     [r.algo_stats.get("n_keys_pruned", "-")     for r in results]),
        ("Refuted by sampling (HyFD)",   [r.algo_stats.get("n_refuted_by_sampling", "-") for r in results]),
    ]
    for label, vals in metrics:
        print(row(label, vals))
    print(f"{'=' * 70}\n")


# ── Persistence ───────────────────────────────────────────────────────────────

def save_results(results: list[AlgoResult], out_path: Path) -> None:
    """
    Write a machine-readable CSV of all discovered FDs with their category.

    Schema: algorithm, lhs (pipe-separated), rhs, category, elapsed_s
    category in {designed_hit, implicit_valid, false_positive, missed}
    """
    rows: list[dict] = []
    for r in results:
        for category, fds in [
            ("designed_hit",   r.designed_hits),
            ("implicit_valid", r.implicit_valid),
            ("false_positive", r.false_positives),
        ]:
            for fd in fds:
                rows.append({
                    "algorithm": r.name,
                    "lhs":       "|".join(sorted(fd.lhs)),
                    "rhs":       fd.rhs,
                    "category":  category,
                    "elapsed_s": f"{r.total_s:.4f}",
                })
        for lhs, rhs in r.missed_gt:
            rows.append({
                "algorithm": r.name,
                "lhs":       "|".join(sorted(lhs)),
                "rhs":       rhs,
                "category":  "missed",
                "elapsed_s": f"{r.total_s:.4f}",
            })

    fieldnames = ["algorithm", "lhs", "rhs", "category", "elapsed_s"]
    with out_path.open("w", newline="", encoding="utf-8") as fh:
        writer = csv.DictWriter(fh, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(rows)
    print(f"Results saved -> {out_path}")


# ── CLI ───────────────────────────────────────────────────────────────────────

def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(
        description="Benchmark HyFD and TANE on the synthetic FD dataset.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    p.add_argument(
        "--data", type=Path, default=DATA_PATH,
        help="Path to the input CSV.",
    )
    p.add_argument(
        "--out", type=Path, default=RESULTS_PATH,
        help="Path for the output results CSV.",
    )
    p.add_argument(
        "--algorithms", nargs="+",
        choices=["HyFD", "TANE"],
        default=["HyFD", "TANE"],
        help="Algorithms to run.",
    )
    p.add_argument(
        "--hyfd-samples", type=int, default=60,
        help="Number of row-pair samples in HyFD Phase 1.",
    )
    p.add_argument(
        "--hyfd-seed", type=int, default=0,
        help="Random seed for HyFD Phase 1 sampling.",
    )
    p.add_argument(
        "--no-save", action="store_true",
        help="Skip saving results CSV.",
    )
    return p.parse_args()


# ── Entry point ───────────────────────────────────────────────────────────────

def main() -> None:
    args = parse_args()

    # ── Dataset ───────────────────────────────────────────────────────────────
    data_path: Path = args.data
    if not data_path.exists():
        print(
            f"[ERROR] Dataset not found: {data_path}\n"
            f"        Generate it first:  python generate_dataset.py",
            file=sys.stderr,
        )
        sys.exit(1)

    df = pd.read_csv(data_path)
    print(f"Dataset  : {data_path.name}  ({len(df):,} rows x {len(df.columns)} columns)")
    print(f"GT FDs   : {len(GROUND_TRUTH)}  (designed ground truth)")
    print(f"Algorithms: {', '.join(args.algorithms)}")

    # ── Algorithm registry ────────────────────────────────────────────────────
    algo_registry: dict[str, tuple[Callable, dict]] = {
        "TANE": (tane.discover, {}),
        "HyFD": (hyfd.discover, {
            "n_sample_pairs": args.hyfd_samples,
            "seed":           args.hyfd_seed,
        }),
    }

    # ── Run each algorithm ────────────────────────────────────────────────────
    results: list[AlgoResult] = []

    for name in args.algorithms:
        discover_fn, kwargs = algo_registry[name]
        print(f"\nRunning {name} ...", flush=True)

        fds, elapsed, algo_stats = run_algorithm(discover_fn, df, kwargs)
        designed_hits, implicit_valid, false_positives, missed_gt = classify(fds, df)

        r = AlgoResult(
            name=name,
            discovered=fds,
            designed_hits=designed_hits,
            implicit_valid=implicit_valid,
            false_positives=false_positives,
            missed_gt=missed_gt,
            algo_stats=algo_stats,
            total_s=elapsed,
        )
        results.append(r)
        print_result(r)

    # ── Side-by-side comparison ───────────────────────────────────────────────
    if len(results) > 1:
        print_comparison(results)

    # ── Persist ───────────────────────────────────────────────────────────────
    if not args.no_save:
        save_results(results, args.out)


if __name__ == "__main__":
    main()
